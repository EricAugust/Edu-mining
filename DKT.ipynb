{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import csv, random\n",
    "import torch.nn.init as init\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import RNN, LSTM, GRU, Sigmoid, Linear, Dropout, Module\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence, pack_sequence\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from sklearn.externals import joblib\n",
    "import time, math\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    从文件中读取数据\n",
    "    :param path: str 文件路径\n",
    "    :return: int 知识点数量, sequence 原始数据序列\n",
    "    \"\"\"\n",
    "    with open(path, 'r', encoding='UTF-8') as f:\n",
    "        skill_dt = {}\n",
    "        skill2name= {}\n",
    "        seq_dt = {}\n",
    "        readlines = csv.reader(f)\n",
    "        for i, readline in enumerate(readlines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            skill_id = readline[1]\n",
    "#             skill_name = readline[2]\n",
    "            user_id = readline[0]\n",
    "            correct = 1 if readline[5] =='True' else 0\n",
    "            if skill_id == '' or user_id == '' or correct == '':\n",
    "                continue\n",
    "#             if skill_id not in skill2name:    \n",
    "#                 skill2name[skill_id] = skill_name\n",
    "            if skill_id not in skill_dt:\n",
    "                skill_dt[skill_id] = len(skill_dt)\n",
    "            if user_id in seq_dt:\n",
    "                seq_dt[user_id].append((skill_dt[(skill_id)], correct))\n",
    "            else:\n",
    "                seq_dt[user_id] = [(skill_dt[(skill_id)], correct)]\n",
    "        seq_dt = {key:value for key, value in seq_dt.items() if len(seq_dt[key]) > 2}\n",
    "        seq_list = list(seq_dt.values())\n",
    "        return skill_dt, seq_list, skill2name\n",
    "\n",
    "\n",
    "def split_dataset(seqs, val_rate=0.2, test_rate=0.2):\n",
    "    \"\"\"\n",
    "    将数据拆分为训练集，验证集和测试集\n",
    "    :param seqs: sequence 原始数据序列\n",
    "    :param val_rate: float 验证集占总体数据比\n",
    "    :param test_rate: float 测试集占总体数据比\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    random.shuffle(seqs)\n",
    "    seq_size = len(seqs)\n",
    "    val_size = int(seq_size * val_rate)\n",
    "    test_size = int(seq_size * test_rate)\n",
    "    val_seqs = seqs[:val_size]\n",
    "    test_seqs = seqs[val_size:val_size + test_size]\n",
    "    train_seqs = seqs[val_size + test_size:]\n",
    "    return train_seqs, val_seqs, test_seqs\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype='int32', padding='pre', truncating='pre', value=0.):\n",
    "    lengths = [len(s) for s in sequences]\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x\n",
    "\n",
    "\n",
    "def num_to_one_hot(num, dim):\n",
    "    base = np.zeros(dim)\n",
    "    if num >= 0:\n",
    "        base[num] += 1\n",
    "    return base\n",
    "\n",
    "class QuizDataSet(Dataset):\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._seq_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self._seq_list[index]\n",
    "\n",
    "    def __init__(self, skill_dt, seq_list):\n",
    "        self._skill_size = len(skill_dt)\n",
    "        self._seq_list = seq_list\n",
    "\n",
    "\n",
    "def collate(batch):\n",
    "    # 压紧序列前需要先排序\n",
    "    batch.sort(key=lambda x: len(x), reverse=True)\n",
    "    seq_len = np.array(list(map(lambda seq: len(seq), batch))) - 1\n",
    "    max_len = max(seq_len) \n",
    "    x = pad_sequences(np.array([[(j[0] + len(skill_dt) * j[1]) for j in i[:-1]] for i in batch]), maxlen=max_len, padding='post', value=-1)\n",
    "    input_x = np.array([[num_to_one_hot(j, len(skill_dt)*2) for j in i] for i in x])\n",
    "    target_id = pad_sequences(np.array([[j[0] for j in i[1:]] for i in batch]), maxlen=max_len, padding='post', value=0)\n",
    "    target_correctness = pad_sequences(np.array([[j[1] for j in i[1:]] for i in batch]), maxlen=max_len, padding='post', value=0)\n",
    "    input_x = torch.Tensor(input_x)\n",
    "    target_id = torch.Tensor(target_id)\n",
    "    target_correctness = torch.Tensor(target_correctness)\n",
    "    return input_x, target_id, target_correctness, seq_len, max_len\n",
    "\n",
    "\n",
    "class DktNet(Module):\n",
    "    \"\"\"\n",
    "    deep knowledge tracing model\n",
    "    input => rnn => dropout => sigmoid => output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, skill_size, rnn_h_size, rnn_layer_size, dropout_rate):\n",
    "        \"\"\"\n",
    "        :param skill_size: int 知识点数量\n",
    "        :param rnn_h_size: int rnn隐藏单元数量\n",
    "        :param rnn_layer_size: int rnn隐藏层数量\n",
    "        :param dropout_rate: float\n",
    "        \"\"\"\n",
    "        super(DktNet, self).__init__()\n",
    "        self.rnn = LSTM(skill_size * 2, rnn_h_size, rnn_layer_size, dropout =dropout_rate, batch_first=True, bias=True)\n",
    "        self.dropout = Dropout(p=dropout_rate)\n",
    "        self.linear = Linear(rnn_h_size, skill_size)\n",
    "        self.sigmoid = Sigmoid()\n",
    "        self.weight_init(self.linear)\n",
    "        self.weight_init(self.rnn)\n",
    "\n",
    "    def weight_init(self, m):\n",
    "        if isinstance(m, nn.LSTM):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.xavier_normal_(m.weight.data)\n",
    "            init.normal_(m.bias.data)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for param in m.parameters():\n",
    "                if len(param.shape) >= 2:\n",
    "                    init.orthogonal_(param.data)\n",
    "                else:\n",
    "                    init.normal_(param.data)\n",
    "    def forward(self, input_data, target_id, sequence_len, max_steps):\n",
    "        input_data = torch.nn.utils.rnn.pack_padded_sequence(input_data, sequence_len, batch_first=True)\n",
    "        state, h = self.rnn(input_data)\n",
    "        state, lengths = torch.nn.utils.rnn.pad_packed_sequence(state, batch_first=True)  # (sequence, lengths)\n",
    "        # rnn_output无需保留序列结构数据，直接用rnn_output.data\n",
    "        dropout_output = self.dropout(state)\n",
    "        logits = self.linear(dropout_output)\n",
    "        pred_all = self.sigmoid(logits)\n",
    "        flat_logits = logits.view(-1)\n",
    "        flat_base_target_index = torch.arange(state.size(0) * max_steps * 1.0) * len(skill_dt)\n",
    "        flat_bias_target_id = target_id.view(-1)\n",
    "        flat_target_id = flat_bias_target_id + flat_base_target_index\n",
    "        flat_target_logits = torch.index_select(flat_logits, 0, flat_target_id.long().cuda())\n",
    "        pred = self.sigmoid(flat_target_logits.view(state.size(0), max_steps))\n",
    "        return state.cpu(), pred.cpu(), flat_target_logits, pred_all\n",
    "\n",
    "\n",
    "def compute_auc(data_loader, model):\n",
    "    \"\"\"\n",
    "    计算验证集和测试集的auc值\n",
    "    \"\"\"\n",
    "    y_pred = torch.tensor([])\n",
    "    y_ans = torch.tensor([])\n",
    "    for x_batch, y_skill_batch, y_ans_batch in data_loader:\n",
    "        skill_pred, hidden = model(x_batch.cuda())\n",
    "        _y_pred = (skill_pred.data * y_skill_batch.cuda()).sum(dim=1)\n",
    "        y_pred = torch.cat((y_pred, _y_pred.cpu()))\n",
    "        y_ans = torch.cat((y_ans, y_ans_batch.cpu()))\n",
    "    return roc_auc_score(y_ans, y_pred)\n",
    "\n",
    "\n",
    "def add_gradient_noise(model, idx_batch, epoch, number_minibatches_per_epoch):\n",
    "    # Adding random gaussian noise as in the paper \"Adding gradient noise improves learning\n",
    "    # for very deep networks\"\n",
    "\n",
    "    for p in model.parameters():\n",
    "        gaussianNoise = torch.Tensor(p.grad.size()).cuda()\n",
    "        # here nu = 0.01, gamma = 0.55, t is the minibatch count = epoch*total_length +idx_minibatch\n",
    "        stdDev = (0.01 / (1 + epoch * number_minibatches_per_epoch + idx_batch) ** 0.55) ** 0.5\n",
    "        gaussianNoise.normal_(0, std=stdDev)\n",
    "        p.grad.data.add_(gaussianNoise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.6 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "# rnn隐藏单元数量\n",
    "HIDDEN_SIZE = 200\n",
    "# rnn隐藏层数量\n",
    "HIDDEN_LAYER_SIZE = 1\n",
    "DROPOUT_RATE = 0.6\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "skill_dt, seq_list, skill2name = read_data('./data/edu_sample.csv')\n",
    "train_seqs, val_seqs, test_seqs = split_dataset(seq_list)\n",
    "train_dataset = QuizDataSet(skill_dt, train_seqs)\n",
    "val_dataset = QuizDataSet(skill_dt, val_seqs)\n",
    "model = DktNet(len(skill_dt), HIDDEN_SIZE, HIDDEN_LAYER_SIZE, DROPOUT_RATE)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(size_average=False)\n",
    "# criterion = torch.nn.NLLLoss( size_average=False)\n",
    "model = model.cuda()\n",
    "criterion = criterion.cuda()\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params=trainable_params, lr=0.001)\n",
    "# optimizer = torch.optim.SGD(params=trainable_params, lr=0.1, momentum=0.7)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=1 / 1.00004)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, collate_fn=collate, shuffle=True, drop_last=True)\n",
    "val_data_loader = DataLoader(val_dataset, collate_fn=collate, batch_size=BATCH_SIZE, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, step: 1, loss: 27574.130859375\n",
      "epoch: 1, step: 51, loss: 1959.206298828125\n",
      "epoch: 1, step: 101, loss: 5768.4609375\n",
      "epoch: 1, step: 151, loss: 35547.859375\n",
      "epoch: 1, step: 201, loss: 39157.359375\n",
      "epoch: 1, step: 251, loss: 3213.887451171875\n",
      "\n",
      " auc=0.6615921981123902, accuracy=0.7974686965746991, precision=[0.32223258 0.83461072], recall=[0.13214886 0.94032004], f_score=[0.18743138 0.88431752]\n",
      "epoch: 2, step: 1, loss: 7463.2919921875\n",
      "epoch: 2, step: 51, loss: 21480.29296875\n",
      "epoch: 2, step: 101, loss: 15460.8994140625\n",
      "epoch: 2, step: 151, loss: 3666.88330078125\n",
      "epoch: 2, step: 201, loss: 5081.5888671875\n",
      "epoch: 2, step: 251, loss: 13456.9365234375\n",
      "\n",
      " auc=0.7013566198032248, accuracy=0.8231763393648132, precision=[0.49660277 0.83346678], recall=[0.08589309 0.98132383], f_score=[0.1464551 0.9013721]\n",
      "epoch: 3, step: 1, loss: 33145.4140625\n",
      "epoch: 3, step: 51, loss: 1107.190673828125\n",
      "epoch: 3, step: 101, loss: 24927.8671875\n",
      "epoch: 3, step: 151, loss: 1282.1593017578125\n",
      "epoch: 3, step: 201, loss: 1161.402587890625\n",
      "epoch: 3, step: 251, loss: 8185.1689453125\n",
      "\n",
      " auc=0.714152432075257, accuracy=0.8256828239932755, precision=[0.54892988 0.83248078], recall=[0.07449339 0.98686549], f_score=[0.13118422 0.9031228 ]\n",
      "epoch: 4, step: 1, loss: 6421.5771484375\n",
      "epoch: 4, step: 51, loss: 4582.32421875\n",
      "epoch: 4, step: 101, loss: 33275.1796875\n",
      "epoch: 4, step: 151, loss: 1130.7979736328125\n",
      "epoch: 4, step: 201, loss: 5262.259765625\n",
      "epoch: 4, step: 251, loss: 9223.3125\n",
      "\n",
      " auc=0.7315139919144885, accuracy=0.8280208211980462, precision=[0.58831818 0.83451108], recall=[0.08780516 0.98681877], f_score=[0.15280458 0.9042966 ]\n",
      "epoch: 5, step: 1, loss: 6548.52734375\n",
      "epoch: 5, step: 51, loss: 14913.6787109375\n",
      "epoch: 5, step: 101, loss: 10142.6201171875\n",
      "epoch: 5, step: 151, loss: 5678.97705078125\n",
      "epoch: 5, step: 201, loss: 708.8040161132812\n",
      "epoch: 5, step: 251, loss: 3056.5439453125\n",
      "\n",
      " auc=0.7477728227192172, accuracy=0.8312889821686992, precision=[0.6227154  0.83827591], recall=[0.11424931 0.98514713], f_score=[0.1930752  0.90579651]\n",
      "epoch: 6, step: 1, loss: 6407.66064453125\n",
      "epoch: 6, step: 51, loss: 2122.263671875\n",
      "epoch: 6, step: 101, loss: 2791.3916015625\n",
      "epoch: 6, step: 151, loss: 4773.228515625\n",
      "epoch: 6, step: 201, loss: 6914.61767578125\n",
      "epoch: 6, step: 251, loss: 3184.187744140625\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (2) : out of memory at c:\\users\\administrator\\downloads\\new-builder\\win-wheel\\pytorch\\aten\\src\\thc\\generic/THCStorage.cu:58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2010cf55f7e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0minput_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_correctness\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msequence_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mflat_target_correctness\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_correctness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mbinary_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-6989fe73e8f1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_data, target_id, sequence_len, max_steps)\u001b[0m\n\u001b[0;32m    166\u001b[0m         \u001b[0mdropout_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdropout_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 168\u001b[1;33m         \u001b[0mpred_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    169\u001b[0m         \u001b[0mflat_logits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[0mflat_base_target_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mmax_steps\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mskill_dt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 242\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cuda runtime error (2) : out of memory at c:\\users\\administrator\\downloads\\new-builder\\win-wheel\\pytorch\\aten\\src\\thc\\generic/THCStorage.cu:58"
     ]
    }
   ],
   "source": [
    "max_val_auc = 0\n",
    "for i in range(EPOCHS):\n",
    "    preds, binary_preds, targets = torch.Tensor([]), torch.Tensor([]), torch.Tensor([])\n",
    "    for j, batched in enumerate(train_data_loader):\n",
    "        scheduler.step()\n",
    "        input_data, target_id, target_correctness, sequence_len, max_steps = batched\n",
    "        optimizer.zero_grad()\n",
    "        _, pred, logits, _  = model(input_data.cuda(), target_id, sequence_len, max_steps)  \n",
    "        flat_target_correctness = target_correctness.view(-1)\n",
    "        binary_pred = torch.gt(pred, 0.5)\n",
    "        loss = criterion(logits, flat_target_correctness.cuda())\n",
    "        \n",
    "        if j % 50 == 0:\n",
    "            print('epoch: {0}, step: {1}, loss: {2}'.format(i + 1, j + 1, loss.item()))\n",
    "        loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm(model.parameters(), 4)\n",
    "\n",
    "        add_gradient_noise(model, j, i, len(train_data_loader))\n",
    "        # gradient clipping\n",
    "        for p in model.parameters():\n",
    "            p.grad = p.grad.clamp(min=-5, max=5)\n",
    "            \n",
    "        optimizer.step()\n",
    "        for seq_idx, seq_len in enumerate(sequence_len):\n",
    "            preds = torch.cat((preds, pred[seq_idx, 0:seq_len].cpu()))\n",
    "            binary_preds = torch.cat((binary_preds, binary_pred[seq_idx, 0:seq_len].float().cpu()))\n",
    "            targets = torch.cat((targets, target_correctness[seq_idx, 0:seq_len].cpu()))\n",
    "    with torch.no_grad():\n",
    "        # compute metrics\n",
    "        preds = preds.detach().numpy()\n",
    "        binary_preds = binary_preds.detach().numpy()\n",
    "        targets = targets.detach().numpy()\n",
    "        auc_value = roc_auc_score(targets, preds)\n",
    "        accuracy = accuracy_score(targets, binary_preds)\n",
    "        precision, recall, f_score, _ = precision_recall_fscore_support(targets, binary_preds)\n",
    "        print(\"\\n auc={0}, accuracy={1}, precision={2}, recall={3}, f_score={4}\".format(auc_value, accuracy, precision, recall,f_score ))\n",
    "        if auc_value > max_val_auc:\n",
    "            max_val_auc = auc_value\n",
    "        else:\n",
    "            print('reduce learning rate by multiply 0.7')\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './chk/dkt.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/edu_sample.csv')\n",
    "data = data[['prerequisites','exercise']].drop_duplicates()\n",
    "data[data['exercise'] == 'represent_by_symbol_addend']['prerequisites'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_dict = {v:k for k,v in skill_dt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test = [[(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)],\n",
    "       [(0, 1), (29, 1), (6, 1), (12, 1), (1, 1), (2, 1), (3, 1), (326, 1), (327, 1), (32, 1), (125, 1)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = [[(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)],\n",
    "       [(0, 0), (29, 0), (6, 0), (12, 0), (1, 0), (2, 0), (3, 0), (326, 0), (327, 0), (32, 0), (125, 0)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[problem_dict[x[0]] for x in test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset1 = QuizDataSet(skill_dt, test1)\n",
    "test_data_loader1 = DataLoader(test_dataset1, batch_size=BATCH_SIZE, collate_fn=collate, shuffle=True, drop_last=True)\n",
    "output1, hidden1 = [], []\n",
    "for j, batched in enumerate(test_data_loader1):\n",
    "    input_data, target_id, target_correctness, sequence_len, max_steps = batched\n",
    "    state, pred, logits, pred_all = model(input_data.cuda(), target_id, sequence_len, max_steps)\n",
    "    hidden1.append(state)\n",
    "    output1.append(pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dataset = QuizDataSet(skill_dt, test)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate, shuffle=True, drop_last=True)\n",
    "output, hidden = [], []\n",
    "for j, batched in enumerate(test_data_loader):\n",
    "    input_data, target_id, target_correctness, sequence_len, max_steps = batched\n",
    "    state, pred, logits, pred_all = model(input_data.cuda(), target_id, sequence_len, max_steps)\n",
    "    hidden.append(state)\n",
    "    output.append(pred_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_skill = [0, 29, 6, 12, 1, 2, 4, 64, 143, 265, 48, 3, 324, 125, 144, 32,  30, 328, 31, 5, 327, 66, 39, 326, 67, 62, 28, 467, 544, 620, 660, 636, 613, 571, 478, 454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(15, 30))\n",
    "df4 = pd.DataFrame(output[0][0][:, selected_skill].data.detach().cpu().numpy())\n",
    "df4.columns = [problem_dict[x] for x in selected_skill]\n",
    "df4.index = [\"({},{})\".format(problem_dict[p], c) for p, c in test[0][:-1]]\n",
    "sns.heatmap(df4.T,cmap='RdBu', vmax=1, vmin=0, annot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 30))\n",
    "df3 = pd.DataFrame(output1[0][0][:, selected_skill].data.detach().cpu().numpy())\n",
    "df3.columns = [problem_dict[x] for x in selected_skill]\n",
    "df3.index = [\"({},{})\".format(problem_dict[p], c) for p, c in test1[0][:-1]]\n",
    "sns.heatmap(df3.T, cmap='RdBu', vmax=1, vmin=0, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "f, (ax1,ax2) = plt.subplots(figsize = (10, 8),nrows=2)\n",
    "df2 = pd.DataFrame(hidden[0][0].data.detach().cpu().numpy())\n",
    "df2.index = [\"({},{})\".format(problem_dict[p], c) for p, c in test[0][:-1]]\n",
    "ax1.set_title('All correct hidden state')\n",
    "ax1.set_xlabel('')\n",
    "ax1.set_xticklabels([]) #设置x轴图例为空值\n",
    "ax1.set_ylabel('kind')\n",
    "sns.heatmap(df2.diff(axis=0).fillna(0.0), cmap='RdBu', vmax=0.5, vmin=-0.5,ax=ax1)\n",
    "df1 = pd.DataFrame(hidden1[0][0].data.detach().cpu().numpy())\n",
    "df1.index = [\"({},{})\".format(problem_dict[p], c) for p, c in test1[0][:-1]]\n",
    "sns.heatmap(df1.diff(axis=0).fillna(0.0), cmap='RdBu', vmax=0.5, vmin=-0.5,ax=ax2)\n",
    "\n",
    "ax2.set_title('All wrong hidden state')\n",
    "ax2.set_xlabel('skill')\n",
    "ax2.set_ylabel('hidden')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(df1.diff(axis=0).fillna(0.0).values - df2.diff(axis=0).fillna(0.0).values), cmap='RdBu', vmax=0.5, vmin=-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import pi\n",
    "import pandas as pd\n",
    "\n",
    "from bokeh.io import show\n",
    "from bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, BasicTicker, PrintfTickFormatter,ColorBar\n",
    "from bokeh.models import FuncTickFormatter\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "data = pd.read_csv('paran-data-df.csv', index_col=0)\n",
    "data = data.T\n",
    "data.index.name = 'cell'\n",
    "data.columns.name = 'chars'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2 = data.diff(axis=1).fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from bokeh.io import output_notebook\n",
    "from bokeh.models import LogColorMapper, LogTicker, ColorBar\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = {i:problem_dict[inputs[i][0]] for i in range(len(data.columns))}\n",
    "\n",
    "seq = [str(i) for i in data.columns]\n",
    "cell = list([str(x) for x in data.index])\n",
    "\n",
    "df = pd.DataFrame(data.stack(), columns=['value']).reset_index()\n",
    "colors = [\"#313695\", \"#4575b4\", \"#74add1\", \"#abd9e9\", \"#e0f3f8\", \"#ffffbf\", \"#fee090\", \"#fdae61\", \"#f46d43\", \"#d73027\", \"#a50026\"]\n",
    "\n",
    "colors.reverse()\n",
    "mapper = LinearColorMapper(palette=colors, low=-1, high=1)#low=df.value.min(), high=df.value.max())\n",
    "source = ColumnDataSource(df)\n",
    "TOOLS = \"hover,pan,reset,save,wheel_zoom\"\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, ticker=BasicTicker(),\n",
    "                     label_standoff=12, border_line_color=None, location=(0,0))\n",
    "\n",
    "p = figure(title=\"LSTM Hidden State Activations\",  x_range=cell, y_range=list(reversed(seq)), x_axis_location=\"above\",\n",
    "            plot_width=500, plot_height=300,\n",
    "            tools=TOOLS, toolbar_location='below')\n",
    "\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.major_label_text_font_size = \"8pt\"\n",
    "p.axis.major_label_standoff = 0\n",
    "p.yaxis.major_label_orientation = pi / 3\n",
    "p.yaxis.formatter = FuncTickFormatter(code=\"\"\"\n",
    "                                        var labels = %s;\n",
    "                                        return labels[tick];\n",
    "                                    \"\"\"%index)\n",
    "\n",
    "p.rect(x=\"cell\", y=\"chars\", width=1, height=1, source=source, fill_color={'field': 'value', 'transform': mapper},\n",
    "                                line_color=None)\n",
    "\n",
    "p.select_one(HoverTool).tooltips = [('value', '@value')]\n",
    "p.add_layout(color_bar, 'right')\n",
    "show(p)      # show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = {i:problem_dict[inputs[i][0]] for i in range(len(data2.columns))}\n",
    "\n",
    "seq = [str(i) for i in data2.columns]\n",
    "cell = list(data2.index)\n",
    "\n",
    "df = pd.DataFrame(data2.stack(), columns=['value']).reset_index()\n",
    "colors = [\"#313695\", \"#4575b4\", \"#74add1\", \"#abd9e9\", \"#e0f3f8\", \"#ffffbf\", \"#fee090\", \"#fdae61\", \"#f46d43\", \"#d73027\", \"#a50026\"]\n",
    "\n",
    "colors.reverse()\n",
    "mapper = LinearColorMapper(palette=colors, low=-1, high=1)#low=df.value.min(), high=df.value.max())\n",
    "source = ColumnDataSource(df)\n",
    "TOOLS = \"hover,pan,reset,save,wheel_zoom\"\n",
    "\n",
    "color_bar = ColorBar(color_mapper=mapper, ticker=BasicTicker(),\n",
    "                     label_standoff=12, border_line_color=None, location=(0,0))\n",
    "\n",
    "p = figure(title=\"LSTM Hidden State Activations\",  x_range=seq, y_range=list(reversed(cell)), x_axis_location=\"above\",\n",
    "            plot_width=300, plot_height=500,\n",
    "            tools=TOOLS, toolbar_location='below')\n",
    "\n",
    "p.grid.grid_line_color = None\n",
    "p.axis.axis_line_color = None\n",
    "p.axis.major_tick_line_color = None\n",
    "p.axis.major_label_text_font_size = \"8pt\"\n",
    "p.axis.major_label_standoff = 0\n",
    "p.xaxis.major_label_orientation = pi / 3\n",
    "p.xaxis.formatter = FuncTickFormatter(code=\"\"\"\n",
    "                                        var labels = %s;\n",
    "                                        return labels[tick];\n",
    "                                    \"\"\"%index)\n",
    "\n",
    "p.rect(x=\"chars\", y=\"cell\", width=1, height=1, source=source, fill_color={'field': 'value', 'transform': mapper},\n",
    "                                line_color=None)\n",
    "\n",
    "p.select_one(HoverTool).tooltips = [('value', '@value')]\n",
    "p.add_layout(color_bar, 'right')\n",
    "show(p)      # show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
